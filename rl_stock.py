# -*- coding: utf-8 -*-
"""RL_Stock.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nxIwn3SKpn2qytY0t87wDal1niVKC7Yn
"""

!pip install gymnasium
!pip install gym_anytrading

# Test run

import gymnasium as gym
from gym_anytrading.datasets import STOCKS_GOOGL
import matplotlib.pyplot as plt

env = gym.make('stocks-v0',
        df=STOCKS_GOOGL,
        window_size=10,
        frame_bound=(10, 300)
    )

observation = env.reset(seed=2024)
while True:
    action = env.action_space.sample()
    observation, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated

    if done:
        print("info:", info)
        break

plt.cla()
env.unwrapped.render_all()
plt.show()

env.action_space

action = env.action_space.sample()
env.reset()
env.step(0)
env.step(1)
env.step(0)
a,b,c,d,info = env.step(action)

env.observation_space

env.action_space

"""###DP"""

def get_reward(env, action, done, actions_done, depth=0, max_depth=10):

  if done or depth>=max_depth:
    return 0

  observation, reward, terminated, truncated, info = env.step(action)
  done = terminated or truncated
  actions_done.append(action) #adding the performed action to the list

  print("actions done: ", actions_done)

  left = get_reward(env, 0, done, actions_done.copy(), depth+1, max_depth) #calculating max total reward when choosing action 0
  #sending copy as an argument to avoid clashes

  env.reset()
  for act in actions_done:
    env.step(act)
  #performing the pervious done actions as the env has now moved to the next actions


  right = get_reward(env, 1, done, actions_done.copy(), depth+1, max_depth) #calculating max reward when choosing action 1

  return reward + max(left, right) #outputting max reward


import gymnasium as gym
from gym_anytrading.datasets import STOCKS_GOOGL
import matplotlib.pyplot as plt

env = gym.make('stocks-v0',
        df=STOCKS_GOOGL,
        window_size=10,
        frame_bound=(10, 300)
    )

observation = env.reset()

actions_done = []


done = False

left = get_reward(env, 0, done, actions_done)

env.reset()
right = get_reward(env, 1, done, actions_done)

reward = max(left, right)

print(reward)



"""###SS DDPGAgent"""

!git clone https://github.com/Sigma-Squad/DDPG-Projects.git

import torch
import torch.nn as nn
import torch.optim as optim
import random


class Actor(nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(Actor, self).__init__()
        self.l1 = nn.Linear(state_dim, hidden_dim)
        self.relu = nn.LeakyReLU()
        self.l2 = nn.Linear(hidden_dim, action_dim)
        self.tanh = nn.Tanh()

    def forward(self, state):
        z = self.relu(self.l1(state))
        # tanh activation for continuous action states
        return self.tanh(self.l2(z))


class Critic(nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(Critic, self).__init__()
        self.l1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.relu = nn.LeakyReLU()
        self.l2 = nn.Linear(hidden_dim, 1)

    def forward(self, state, action):
        z = torch.cat([state, action], dim=1)
        z = self.relu(self.l1(z))
        return self.l2(z)


class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []
        self.position = 0

    def push(self, state, action, reward, next_state, done):
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        self.memory[int(self.position)] = (
            state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        return zip(*random.sample(self.memory, batch_size))


class DDPGAgent:
    def __init__(self, state_dim, hidden_dim, action_dim, replay_buffer_size=1e6, gamma=0.99, tau=0.001, batch_size=8):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.replay_buffer = ReplayBuffer(replay_buffer_size)
        self.gamma = gamma  # discount factor
        self.tau = tau  # learning rate
        self.batch_size = batch_size

        # Actor Network
        self.actor = Actor(state_dim, hidden_dim, action_dim)
        self.target_actor = Actor(state_dim, hidden_dim, action_dim)
        self.actor_optimizer = optim.Adam(self.actor.parameters())

        # Critic network
        self.critic = Critic(state_dim, hidden_dim, action_dim)
        self.target_critic = Critic(state_dim, hidden_dim, action_dim)
        self.critic_optimizer = optim.Adam(self.critic.parameters())

    def act(self, state):
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        action = self.actor(state)
        return action.detach().numpy()[0]

    def remember(self, state, action, reward, next_state, done):
        self.replay_buffer.push(state, action, reward, next_state, done)

    def train(self):
        torch.autograd.set_detect_anomaly(True)
        if self.replay_buffer.capacity < self.batch_size or len(self.replay_buffer.memory) < self.batch_size:
            return

        states, actions, rewards, next_states, dones = self.replay_buffer.sample(
            self.batch_size)
        states = torch.tensor(states, dtype=torch.float32)
        actions = torch.tensor(actions, dtype=torch.float32)
        rewards = torch.tensor(rewards, dtype=torch.float32)
        next_states = torch.tensor(next_states, dtype=torch.float32)
        dones = torch.tensor(dones, dtype=torch.float32)

        y_Q = torch.add(rewards, self.gamma *
                        self.target_critic(next_states, self.target_actor(next_states)))

        critic_loss = nn.MSELoss()(y_Q, self.critic(states, actions))
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        actor_loss = -torch.mean(self.critic(states, self.actor(states)))
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        self._update_target_networks(self.target_actor, self.actor, self.tau)
        self._update_target_networks(self.target_critic, self.critic, self.tau)

    def _update_target_networks(self, target_model, model, tau):
        for target_param, param in zip(target_model.parameters(), model.parameters()):
            target_param.data.copy_(
                tau * param.data + (1 - tau) * target_param.data)

import gymnasium as gym
from gym_anytrading.datasets import STOCKS_GOOGL
import numpy as np
import matplotlib.pyplot as plt

env = gym.make('stocks-v0',
        df=STOCKS_GOOGL,
        window_size=10,
        frame_bound=(10, 300)
    )

env = gym.make('Pendulum-v1', g=9.81)

env.observation_space.shape

state_dim = 20
hidden_dim = 8
action_dim = 1

model = DDPGAgent(state_dim, hidden_dim, action_dim)

episodes = 2
total_rewards = []

for episode in range(episodes):
  state, _ = env.reset()
  state = np.array(state).reshape(-1)
  current_episode_reward = 0
  done = False

  while not done:
    action = model.act(state)
    # print(action)

    next_state, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    next_state = np.array(next_state).reshape(-1)
    model.remember(state, action, reward, next_state, done)
    state = next_state
    model.train()
    current_episode_reward += reward


  print(f"{episode}: ep reward {info}")

  total_rewards.append(current_episode_reward)

env.history



"""###Pendulum DDPG"""

from stable_baselines3 import DDPG

env = gym.make('Pendulum-v1', g=9.81)

model = DDPG('MlpPolicy', env, verbose=1)

total_rewards = []

for episode in range(0, 10):
  vec_env = model.get_env()
  obs = vec_env.reset()
  done = False

  model.learn(total_timesteps=10_000)

  reward_per_episode = 0

  while not done:
    action, _states = model.predict(obs)
    obs, reward, done, info = vec_env.step(action)

    reward_per_episode+=reward

  total_rewards.append(reward_per_episode)

  print(f"{episode}: ep reward {reward_per_episode}")

print(f"mean: {np.mean(total_rewards)}, max: {np.max(total_rewards)}, min: {np.min(total_rewards)}")

print(total_rewards)



"""###Pendulum DDPGAgent"""

env = gym.make('Pendulum-v1', g=9.81)

env.observation_space.shape

state_dim = 3
hidden_dim = 4
action_dim = 1

model = DDPGAgent(state_dim, hidden_dim, action_dim, batch_size=4, gamma=0.99, tau=0.01)

env.reset()

episodes = 10
rewards = []

for episode in range(episodes):
  state, reward = env.reset()
  state = np.array(state).reshape(-1)
  done = False

  reward_ep=0

  while not done:
    action = model.act(state)
    # action = 1 if action>=0.5 else 0

    next_state, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    next_state = np.array(next_state).reshape(-1)
    model.remember(state, action, reward, next_state, done)
    state = next_state
    model.train()
    reward_ep+=reward

  print(f"reward: {reward_ep}, episode: {episode}")
  rewards.append(reward_ep)

print(f"mean: {np.mean(rewards)}, min: {np.min(rewards)}, max: {np.max(rewards)}")

"""###Stock DDPGAgent"""

env = gym.make('stocks-v0',
        df=STOCKS_GOOGL,
        window_size=10,
        frame_bound=(10, 300)
    )

env.observation_space.shape

env.action_space.n

state_dim = 20
hidden_dim = 5
action_dim = 1

model = DDPGAgent(state_dim, hidden_dim, action_dim, batch_size=4)

episodes = 5
rewards = []

for episode in range(episodes):
  state, reward = env.reset()
  state = np.array(state).reshape(20)

  done = False
  reward_ep = 0

  while not done:
    action = model.act(state)
    print(action<0.5)
    action = 0 if action<=0.5 else 1

    next_state, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated

    next_state = np.array(next_state).reshape(20)

    model.remember(state, [action], reward, next_state, done)
    model.train()
    state = next_state
    reward_ep+=reward

  print(f"reward: {reward_ep}, episode: {episode}")
  rewards.append(reward_ep)

